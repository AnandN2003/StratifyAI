## StratifyAI - Autonomous Company Research Agent

[![Python](https://img.shields.io/badge/python-3.9-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-enabled-brightgreen.svg)](https://www.docker.com/)
[![LangGraph](https://img.shields.io/badge/LangGraph-orchestration-orange.svg)](https://github.com/langchain-ai/langgraph)
[![Gemini](https://img.shields.io/badge/Gemini-2.5%20Flash-purple.svg)](https://ai.google.dev/)

> **An intelligent AI agent that autonomously researches companies, detects conflicting information, and generates professional account plans for sales due diligence.**

---

## ğŸ“– Overview

StratifyAI is an **Agentic AI system** built with LangGraph that combines web search, LLM-powered analysis, and human-in-the-loop decision making to create comprehensive company research reports. Perfect for sales teams, investors, and business analysts who need fast, reliable company intelligence.

### Key Features

- **Autonomous Web Research** - Searches 12+ sources per company using Tavily API
- **AI-Powered Conflict Detection** - Gemini 2.5 Flash identifies factual discrepancies
- **Human-in-the-Loop** - Interactive prompts for critical decisions when conflicts arise
- **Structured Account Plans** - Professional reports with SWOT analysis and sales insights
- **Fully Dockerized** - Reproducible environment, runs anywhere
- **Fast Execution** - Complete research and report in 30-45 seconds

---

## Architecture

```
User Input â†’ Researcher â†’ Reviewer â†’ [Human Review?] â†’ Writer â†’ Account Plan
              (Tavily)    (Gemini)    (Interactive)    (Gemini)
```

**4-Node LangGraph Workflow:**
1. **Researcher Node** - 4 targeted web searches (12 sources total)
2. **Reviewer Node** - AI fact-checking for conflicts
3. **Human Node** - Interactive decision making when needed
4. **Writer Node** - Professional report generation


## Quick Start

### Prerequisites
- Docker and Docker Compose installed
- API keys for Gemini and Tavily (in `.env` file)

### Running the Application

1. **Build and start the application:**
   ```bash
   docker-compose up --build
   ```

2. **Access the application:**
   - Open your browser and navigate to: `http://localhost:8501`
   - You'll see the landing page
   - Click "Get Started" to access the chat interface

3. **Stop the application:**
   ```bash
   docker-compose down
   ```

## ğŸ“ Project Structure

```
StratifyAI/
â”œâ”€â”€ app/                        # Streamlit application
â”‚   â”œâ”€â”€ landing.py              # Landing page (main entry point)
â”‚   â”œâ”€â”€ streamlit_app.py        # Original research interface
â”‚   â”œâ”€â”€ streamlit_chat.py       # Alternative chat interface
â”‚   â”œâ”€â”€ .streamlit/
â”‚   â”‚   â””â”€â”€ config.toml         # Streamlit configuration
â”‚   â””â”€â”€ pages/
â”‚       â””â”€â”€ chat.py             # Chat interface page
â”œâ”€â”€ frontend/                   # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx             # Main App component
â”‚   â”‚   â”œâ”€â”€ Chat.jsx            # Chat interface component
â”‚   â”‚   â”œâ”€â”€ Landing.jsx         # Landing page component
â”‚   â”‚   â”œâ”€â”€ main.jsx            # React entry point
â”‚   â”‚   â”œâ”€â”€ App.css             # App styles
â”‚   â”‚   â”œâ”€â”€ Chat.css            # Chat styles
â”‚   â”‚   â””â”€â”€ index.css           # Global styles
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ index.html          # HTML template
â”‚   â”œâ”€â”€ Dockerfile              # Frontend Docker configuration
â”‚   â”œâ”€â”€ nginx.conf              # Nginx configuration
â”‚   â””â”€â”€ vite.config.js          # Vite bundler configuration
â”œâ”€â”€ src/                        # Core AI agent logic
â”‚   â”œâ”€â”€ graph.py                # LangGraph agent workflow
â”‚   â”œâ”€â”€ entrypoint.py           # CLI entry point
â”‚   â””â”€â”€ __init__.py             # Package initialization
â”œâ”€â”€ backend_api.py              # Flask API backend
â”œâ”€â”€ docker-compose.yml          # Multi-container orchestration
â”œâ”€â”€ Dockerfile                  # Backend Docker configuration
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .gitignore                  # Git ignore rules
â””â”€â”€ README.md                   # Project documentation
```

## Features

- **AI-Powered Research**: Leverages advanced AI to analyze companies instantly
- **Comprehensive Account Plans**: Generates detailed plans with key insights
- **Lightning Fast**: Get results in minutes instead of hours
- **Smart Targeting**: Identify decision-makers and organizational structure
- **Strategic Insights**: Uncover growth initiatives and strategic priorities
- **Conversational Interface**: Natural chat-based interaction
- **Export Options**: Download reports as Markdown or PDF

## ğŸ”§ Configuration

The application is configured through environment variables in the `.env` file:
- `GEMINI_API_KEY`: Your Google Gemini API key
- `TAVILY_API_KEY`: Your Tavily search API key

## ğŸ“ Usage

1. Start on the landing page
2. Click "Get Started" to access the chat
3. Enter a company name to research
4. Review the AI-generated account plan
5. Download the report in your preferred format (MD or PDF)

---
## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Orchestration** | LangGraph | State management & workflow routing |
| **LLM** | Google Gemini 2.5 Flash | Analysis & report generation |
| **Search** | Tavily API | Web research (12 sources/company) |
| **Backend** | Python 3.9 | Core logic |
| **Container** | Docker | Reproducible environment |
| **Framework** | LangChain | LLM integration |

---

## Use Cases

- **Sales Teams** - Pre-call research and account planning
- **Investment Analysts** - Company due diligence
- **Business Development** - Partner evaluation
- **Consultants** - Client background research
- **Market Research** - Competitive intelligence

---

## ğŸ¬ How It Works

### 1. Research Phase
The **Researcher Node** executes 4 targeted queries:
- Company overview and business model
- Recent news and developments
- Products and services
- Market position and competitors

Each query returns 3 sources = **12 total web articles**

### 2. Conflict Detection
The **Reviewer Node** sends all findings to Gemini AI:
- Checks for numerical discrepancies (revenue, employees)
- Identifies conflicting facts (CEO names, headquarters)
- Detects mixed company identities (same acronym)
- Returns structured JSON with conflict status

### 3. Human Decision (If Needed)
When conflicts detected:
```
âš ï¸ CONFLICT DETECTED - HUMAN REVIEW REQUIRED

Finding 2 lists headquarters as "Dublin, Ireland, UK" while 
Finding 1 states "Dublin, Ireland." Please clarify...

Options:
  1. Proceed anyway (ignore conflict)
  2. Stop and review manually
  3. Add clarification note

Your decision (1/2/3):
```

### 4. Report Generation
The **Writer Node** synthesizes validated research into:
- Executive Summary
- Key Financial & Operational Insights
- SWOT Analysis (formatted table)
- 3 Sales Conversation Starters

---
## Roadmap

- [ ] Streamlit Web UI
- [ ] FastAPI REST backend
- [ ] PostgreSQL for research history
- [ ] Multi-company batch processing
- [ ] PDF export functionality
- [ ] Custom search provider integration
- [ ] Supervisor node for multi-agent orchestration

---
## Acknowledgments

- **LangGraph** - State machine orchestration
- **Google Gemini** - AI analysis and generation
- **Tavily** - Web search API
- **LangChain** - LLM framework
